{"cells":[{"cell_type":"markdown","metadata":{"id":"Xgux6NIOe_nv"},"source":[]},{"cell_type":"markdown","metadata":{"id":"agBiQu_1e_n3"},"source":["### News"]},{"cell_type":"markdown","metadata":{"id":"5ZHXmNnde_n4"},"source":[]},{"cell_type":"markdown","source":["Unsloth library and its dependencies are installed correctly, whether you're running it in a Google Colab/Kaggle notebook or on your local machine. It intelligently adapts the installation process based on the environment, streamlining setup for users."],"metadata":{"id":"jkQwDRKapsl7"}},{"cell_type":"markdown","metadata":{"id":"l9OCGhjHe_n5"},"source":["### Installation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"46kGc4Lre_n7","executionInfo":{"status":"ok","timestamp":1740233331277,"user_tz":0,"elapsed":24769,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ffa23d94-b081-40a2-ac16-2682ef64f510"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting bitsandbytes\n","  Downloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n","Collecting xformers==0.0.29\n","  Downloading xformers-0.0.29-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n","Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n","Collecting trl\n","  Downloading trl-0.15.1-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.1.0)\n","Downloading xformers-0.0.29-cp311-cp311-manylinux_2_28_x86_64.whl (15.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trl-0.15.1-py3-none-any.whl (318 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xformers, trl, bitsandbytes\n","Successfully installed bitsandbytes-0.45.2 trl-0.15.1 xformers-0.0.29\n","Collecting cut_cross_entropy\n","  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n","Collecting unsloth_zoo\n","  Downloading unsloth_zoo-2025.2.7-py3-none-any.whl.metadata (16 kB)\n","Downloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n","Downloading unsloth_zoo-2025.2.7-py3-none-any.whl (107 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.1/107.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: unsloth_zoo, cut_cross_entropy\n","Successfully installed cut_cross_entropy-25.1.1 unsloth_zoo-2025.2.7\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (4.25.6)\n","Collecting datasets\n","  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.28.1)\n","Collecting hf_transfer\n","  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.12.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, hf_transfer, dill, multiprocess, datasets\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","unsloth-zoo 2025.2.7 requires tyro, which is not installed.\n","unsloth-zoo 2025.2.7 requires protobuf<4.0.0, but you have protobuf 4.25.6 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.3.2 dill-0.3.8 hf_transfer-0.1.9 multiprocess-0.70.16 xxhash-3.5.0\n","Collecting unsloth\n","  Downloading unsloth-2025.2.15-py3-none-any.whl.metadata (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading unsloth-2025.2.15-py3-none-any.whl (188 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.8/188.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: unsloth\n","Successfully installed unsloth-2025.2.15\n"]}],"source":["\n","    # Do this only in Colab and Kaggle notebooks! Otherwise use pip install unsloth\n","!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n","!pip install --no-deps cut_cross_entropy unsloth_zoo\n","!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n","!pip install --no-deps unsloth"]},{"cell_type":"markdown","source":["**from unsloth import FastLanguageModel**: This line imports the FastLanguageModel class from the unsloth library. This class is central to working with the language models provided by Unsloth.\n","\n","**import torch**: This line imports the torch library, which is a fundamental library for deep learning in Python.\n"],"metadata":{"id":"dmjsBLT5p_PN"}},{"cell_type":"markdown","source":["**max_seq_length** = 2048: This sets the maximum sequence length for the model's input and output. It determines how many tokens the model can process at once.\n","\n","**dtype** = None: This sets the data type for the model's weights. None enables automatic detection based on your hardware. You can manually set it to Float16 or Bfloat16 for specific hardware like Tesla T4/V100 or Ampere GPUs, respectively.\n","\n","**load_in_4bit** = True: This enables 4-bit quantization, a technique to reduce the model's memory footprint, leading to faster loading and potentially allowing you to use larger models on your hardware."],"metadata":{"id":"1jutzI5Cp_ZP"}},{"cell_type":"markdown","source":["**fourbit_models**: This is a list of pre-quantized models available in the Unsloth library. These models are optimized for faster downloading and reduced memory consumption."],"metadata":{"id":"u5VPtt9Fp_kE"}},{"cell_type":"markdown","source":["**model, tokenizer** = FastLanguageModel.from_pretrained(...): This line loads the specified pre-trained language model and its associated tokenizer using the from_pretrained method of the FastLanguageModel class.\n","\n","**model_name** = \"unsloth/Llama-3.2-3B-Instruct\": This specifies the name of the model to load. You can choose from the models listed in fourbit_models or other compatible models.\n","\n","**max_seq_length, dtype, load_in_4bit**: These parameters are passed to the from_pretrained method to configure the model loading process."],"metadata":{"id":"dxZJUm97p_ru"}},{"cell_type":"markdown","source":["this code snippet imports necessary libraries, sets parameters for model configuration, provides a list of available pre-quantized models, and finally loads the selected language model along with its tokenizer for use in subsequent tasks."],"metadata":{"id":"664hgVkwp_w7"}},{"cell_type":"markdown","metadata":{"id":"HA0Dypn1e_n-"},"source":["### Unsloth"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bmNa2Ahwe_n_","outputId":"faac5da3-f8e5-4b96-ec87-5f55e84e96ab","executionInfo":{"status":"ok","timestamp":1740234501964,"user_tz":0,"elapsed":20141,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.48.3.\n","   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29. FA2 = False]\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]}],"source":["from unsloth import FastLanguageModel\n","import torch\n","max_seq_length = 2048\n","dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n","load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n","\n","\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit,\n","    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",")"]},{"cell_type":"markdown","source":["This code is focused on efficiently fine-tuning a pre-trained language model using a technique called LoRA **(Low-Rank Adaptation)**. LoRA is a method that allows you to fine-tune large language models with significantly fewer trainable parameters, making the process faster and requiring less memory."],"metadata":{"id":"SzfUmd0wqnna"}},{"cell_type":"markdown","source":["**model**= FastLanguageModel.get_peft_model(...): This line calls the get_peft_model function from the FastLanguageModel class. This function is responsible for adding LoRA adapters to the existing pre-trained model.\n","\n","**model**: This is the pre-trained language model that you loaded earlier using FastLanguageModel.from_pretrained. It's the base model you want to fine-tune.\n","\n","**r** = 16: This parameter (r) controls the rank of the LoRA adapters. It's a hyperparameter that you can adjust, and it affects the number of trainable parameters introduced by LoRA. The code suggests some typical values (8, 16, 32, 64, 128). A higher rank generally leads to better performance but increases the number of trainable parameters.\n","\n","**target_modules** = [...]: This list specifies the names of the modules within the base model where you want to apply LoRA adapters. These modules are typically related to the attention mechanism in the transformer architecture.\n","\n","**lora_alpha** = 16: This parameter (lora_alpha) is a scaling factor used in the LoRA update. It helps control the impact of the LoRA adapters on the model's output.\n","\n","**lora_dropout** = 0: This sets the dropout rate for the LoRA adapters. Dropout is a regularization technique used to prevent overfitting during training. While it supports any value, 0 is optimized for performance in this case.\n","\n","**bias** = \"none\": This parameter controls whether to add a bias term to the LoRA adapters. It is set to \"none\" for optimization purposes.\n","\n","**use_gradient_checkpointing **= \"unsloth\": This option enables gradient checkpointing, a technique to reduce memory consumption during training, especially for models with long context. The \"unsloth\" setting is a specific implementation optimized for Unsloth's library.\n","\n","**random_state** = 3407: This is used to seed the random number generator, ensuring reproducibility of your experiments.\n","\n","**use_rslora **= False: This indicates whether to use rank-stabilized LoRA (RS-LoRA), a variation of LoRA. It is set to False in this example.\n","\n","**loftq_config** = None: This parameter is related to another quantization technique called LoftQ. It's set to None, meaning it's not used in this case."],"metadata":{"id":"B4ttJSEwqnvU"}},{"cell_type":"markdown","source":["this code snippet adds LoRA adapters to your pre-trained language model to prepare it for efficient fine-tuning. This enables you to adapt the model for specific tasks with minimal changes to the base model's architecture while using fewer resources compared to traditional fine-tuning methods."],"metadata":{"id":"gJDYE4tcqnyg"}},{"cell_type":"markdown","metadata":{"id":"SXd9bTZd1aaL"},"source":["We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6bZsfBuZDeCL"},"outputs":[],"source":["model = FastLanguageModel.get_peft_model(\n","    model,\n","    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n","    lora_alpha = 16,\n","    lora_dropout = 0, # Supports any, but = 0 is optimized\n","    bias = \"none\",    # Supports any, but = \"none\" is optimized\n","    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n","    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n","    random_state = 3407,\n","    use_rslora = False,  # We support rank stabilized LoRA\n","    loftq_config = None, # And LoftQ\n",")"]},{"cell_type":"markdown","metadata":{"id":"vITh0KVJ10qX"},"source":["<a name=\"Data\"></a>\n","### Data Prep\n","Use the `Llama-3.1` format for conversation style finetunes. We use [Maxime Labonne's FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset in ShareGPT style. But we convert it to HuggingFace's normal multiturn format `(\"role\", \"content\")` instead of `(\"from\", \"value\")`/ Llama-3 renders multi turn conversations like below:\n","\n","```\n","<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n","\n","Hello!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\n","Hey there! How are you?<|eot_id|><|start_header_id|>user<|end_header_id|>\n","\n","I'm great thanks!<|eot_id|>\n","```\n","\n","`get_chat_template` function to get the correct chat template. We support `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, phi3, llama3` and more."]},{"cell_type":"markdown","source":["This code defines a function called formatting_prompts_func. This function's primary purpose is to take a batch of examples from a dataset and format them for the language model to understand."],"metadata":{"id":"JMc5i4fOrJHD"}},{"cell_type":"markdown","source":["**def formatting_prompts_func(examples)**:: This line defines the function named formatting_prompts_func and indicates that it takes one argument called examples. examples is expected to be a dictionary-like object (a batch of data) containing the conversations.\n","\n","**convos **= examples[\"conversations\"]:: This line extracts the conversations from the examples dictionary and assigns them to the convos variable. The \"conversations\" key is assumed to exist within the examples dictionary and hold the actual conversation data.\n","\n","**texts** = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]: This line is more complex. Let's break it down further:\n","\n","It uses a list comprehension to iterate through each convo in the convos list.\n","For each convo, it calls the apply_chat_template method of the tokenizer object.\n","apply_chat_template is a function that formats the conversation (convo) into a specific structure that the language model expects.\n","\n","**tokenize** = False indicates that the conversation shouldn't be tokenized yet.\n","add_generation_prompt = False means that special tokens used for prompting the model to generate text aren't added at this stage.\n","Finally, the formatted text is added to the texts list.\n","\n","**return { \"text\" : texts, }**: After processing all conversations, the function returns a dictionary where the key is \"text\" and the value is the texts list containing the formatted conversations.\n","\n","\n","**pass**: This line is essentially a placeholder. It doesn't do anything but prevents an error if the function were empty. It's often used during development when you're still working on the logic within a function."],"metadata":{"id":"HHWZuRgxrJVB"}},{"cell_type":"markdown","source":["In essence, this function takes raw conversation data, applies a specific chat template to format it, and then returns the formatted data ready to be used by the language model."],"metadata":{"id":"MmgrLfrnrJbT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LjY75GoYUCB8"},"outputs":[],"source":["from unsloth.chat_templates import get_chat_template\n","\n","tokenizer = get_chat_template(\n","    tokenizer,\n","    chat_template = \"llama-3.1\",\n",")\n","\n","def formatting_prompts_func(examples):\n","    convos = examples[\"conversations\"]\n","    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n","    return { \"text\" : texts, }\n","pass\n","\n","from datasets import load_dataset\n","dataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")"]},{"cell_type":"code","source":["dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Npy3qkJUAnpN","executionInfo":{"status":"ok","timestamp":1740234707952,"user_tz":0,"elapsed":9,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"outputId":"1be3c85d-5f18-47b0-ece2-8e4f06bd9d0e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['conversations', 'source', 'score'],\n","    num_rows: 100000\n","})"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"K9CBpiISFa6C"},"source":[" `standardize_sharegpt` to convert ShareGPT style datasets into HuggingFace's generic format. This changes the dataset from looking like:\n","```\n","{\"from\": \"system\", \"value\": \"You are an assistant\"}\n","{\"from\": \"human\", \"value\": \"What is 2+2?\"}\n","{\"from\": \"gpt\", \"value\": \"It's 4.\"}\n","```\n","to\n","```\n","{\"role\": \"system\", \"content\": \"You are an assistant\"}\n","{\"role\": \"user\", \"content\": \"What is 2+2?\"}\n","{\"role\": \"assistant\", \"content\": \"It's 4.\"}\n","```"]},{"cell_type":"markdown","source":["This section of the code deals with preparing the dataset for training  the language model. It involves two main steps: standardization and formatting."],"metadata":{"id":"LpgFVLA9rp29"}},{"cell_type":"markdown","source":["**from unsloth.chat_templates import standardize_sharegpt**: This line imports the standardize_sharegpt function from the unsloth.chat_templates module. This function is specifically designed to handle datasets in the ShareGPT format.\n","\n","**dataset** = standardize_sharegpt(dataset): This line applies the standardize_sharegpt function to the dataset variable. It converts the dataset from the ShareGPT format (using \"from\" and \"value\" keys) to the Hugging Face format (using \"role\" and \"content\" keys)."],"metadata":{"id":"V_Vly9yErp8k"}},{"cell_type":"markdown","source":["This standardization step ensures that the dataset is compatible with the expected input format of the Hugging Face language model."],"metadata":{"id":"5JqJyPWmrqBA"}},{"cell_type":"markdown","source":["**dataset = dataset.map(...)**: The map function is a common operation in dataset processing. It applies a given function (formatting_prompts_func in this case) to each element of the dataset.\n","\n","**formatting_prompts_func**: This is a custom function (defined earlier in the code) that takes a batch of conversation data and formats it according to the specific template required by the language model.\n","\n","**batched** = True: This argument indicates that the formatting_prompts_func should be applied to batches of data rather than individual elements. This can significantly speed up the proce"],"metadata":{"id":"rYojF0qCr6jA"}},{"cell_type":"markdown","source":[" this part of the code standardizes the dataset to a common format and then applies a specific formatting function to structure the conversations for the language model's understanding. This preparation is crucial for successful fine-tuning."],"metadata":{"id":"kSy65ZR5r6oV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oPXzJZzHEgXe"},"outputs":[],"source":["from unsloth.chat_templates import standardize_sharegpt\n","dataset = standardize_sharegpt(dataset)\n","dataset = dataset.map(formatting_prompts_func, batched = True,)"]},{"cell_type":"markdown","metadata":{"id":"ndDUB23CGAC5"},"source":["We look at how the conversations are structured for item 5:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gGFzmplrEy9I","executionInfo":{"status":"ok","timestamp":1740234782760,"user_tz":0,"elapsed":8,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"44c3919f-f3ac-42d4-8afb-7adc58cb922b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'content': 'How do astronomers determine the original wavelength of light emitted by a celestial body at rest, which is necessary for measuring its speed using the Doppler effect?',\n","  'role': 'user'},\n"," {'content': 'Astronomers make use of the unique spectral fingerprints of elements found in stars. These elements emit and absorb light at specific, known wavelengths, forming an absorption spectrum. By analyzing the light received from distant stars and comparing it to the laboratory-measured spectra of these elements, astronomers can identify the shifts in these wavelengths due to the Doppler effect. The observed shift tells them the extent to which the light has been redshifted or blueshifted, thereby allowing them to calculate the speed of the star along the line of sight relative to Earth.',\n","  'role': 'assistant'}]"]},"metadata":{},"execution_count":16}],"source":["dataset[5][\"conversations\"]"]},{"cell_type":"markdown","metadata":{"id":"GfzTdMtvGE6w"},"source":["And we see how the chat template transformed these conversations.\n","\n","**[Notice]** Llama 3.1 Instruct's default chat template default adds `\"Cutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\"`, so do not be alarmed!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vhXv0xFMGNKE","executionInfo":{"status":"ok","timestamp":1740234806136,"user_tz":0,"elapsed":46,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"colab":{"base_uri":"https://localhost:8080/","height":125},"outputId":"31b7e5b3-9335-400b-c91b-084f688be88e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow do astronomers determine the original wavelength of light emitted by a celestial body at rest, which is necessary for measuring its speed using the Doppler effect?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAstronomers make use of the unique spectral fingerprints of elements found in stars. These elements emit and absorb light at specific, known wavelengths, forming an absorption spectrum. By analyzing the light received from distant stars and comparing it to the laboratory-measured spectra of these elements, astronomers can identify the shifts in these wavelengths due to the Doppler effect. The observed shift tells them the extent to which the light has been redshifted or blueshifted, thereby allowing them to calculate the speed of the star along the line of sight relative to Earth.<|eot_id|>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}],"source":["dataset[5][\"text\"]"]},{"cell_type":"markdown","source":["This section is where the actual training of the language model takes place using the Hugging Face Transformers and TRL (Transformer Reinforcement Learning) libraries."],"metadata":{"id":"xaG5U3Q4smXp"}},{"cell_type":"markdown","source":["**from trl import SFTTrainer**: This line imports the SFTTrainer class from the trl library. SFTTrainer is a specialized trainer for Supervised Fine-Tuning (SFT) of language models. It provides a framework for training language models on a dataset of input-output pairs."],"metadata":{"id":"3_TwYq2-sme_"}},{"cell_type":"markdown","source":["**from transformers import TrainingArguments, DataCollatorForSeq2Seq**: This imports necessary components from the transformers library:\n","\n","**TrainingArguments**: This class holds all the hyperparameters and configurations for the training process (learning rate, batch size, number of epochs, etc.).\n","\n","**DataCollatorForSeq2Seq**: This is used to prepare the data for input to the model during training, specifically for sequence-to-sequence tasks like chat. It handles padding, tokenization, and formatting of inputs and targets.\n"],"metadata":{"id":"v5mLr3OOs9U-"}},{"cell_type":"markdown","source":["**from unsloth import is_bfloat16_supported**: This imports a utility function is_bfloat16_supported from the unsloth library. This function checks if your hardware supports the bfloat16 data type, which can be more efficient for training on certain GPUs."],"metadata":{"id":"9M09_aS4smiP"}},{"cell_type":"markdown","source":["**Trainer Initialization**\n"],"metadata":{"id":"8pV_9EkQsmlp"}},{"cell_type":"markdown","source":["**trainer = SFTTrainer(...)**: This line creates an instance of the SFTTrainer class, which will manage the training process. Let's examine the arguments provided:\n","\n","**model = model**: This passes the pre-trained language model (with LoRA adapters) that you loaded and configured earlier. This is the model that will be fine-tuned.\n","\n","**tokenizer = tokenizer**: This provides the tokenizer associated with the model. The tokenizer is used to convert text into numerical representations that the model can understand.\n","\n","**train_dataset = dataset**: This specifies the dataset that will be used for training. This dataset should contain conversations or text examples in the format expected by the model.\n","\n","**dataset_text_field = \"text\"**: This tells the trainer where to find the text data within each example in the dataset. In this case, it's assumed that the text is stored under the key \"text\".\n","\n","\n","**max_seq_length = max_seq_length**: This sets the maximum sequence length for the input to the model during training. Sequences longer than this will be truncated.\n","\n","\n","**data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer)**: This provides the data collator, which prepares the data for input to the model. It uses the specified tokenizer.\n","\n","**dataset_num_proc = 2**: This sets the number of processes to use for data preprocessing. Using multiple processes can speed up data loading.\n","\n","**packing = False**: This controls whether to pack multiple shorter sequences into a single batch. Packing can improve training speed but is disabled here.\n","\n","**args = TrainingArguments(...)**: This passes an instance of TrainingArguments containing all the hyperparameters for the training process. Let's break down the important ones:\n","\n","**per_device_train_batch_size = 2**: The number of training examples to process in each batch on each device (GPU).\n","\n","**gradient_accumulation_steps = 4**: The number of steps to accumulate gradients before updating the model's weights. This effectively increases the batch size without requiring more GPU memory.\n","\n","**warmup_steps = 5**: The number of initial steps where the learning rate is gradually increased.\n","\n","**max_steps = 60**: The total number of training steps to perform.\n","\n","**learning_rate = 2e-4**: The learning rate for the optimizer.\n","\n","**fp16 = not is_bfloat16_supported()**: Whether to use 16-bit floating-point precision for training. This can speed up training on compatible GPUs.\n","\n","**bf16 = is_bfloat16_supported()**: Whether to use bfloat16 precision if supported by your hardware.\n","\n","**logging_steps = 1**: How often to log training progress (every step in this case).\n","\n","**optim = \"adamw_8bit\"**: The optimizer to use for training (AdamW with 8-bit optimization).\n","\n","**weight_decay = 0.01**: A regularization technique to prevent overfitting.\n","lr_scheduler_type = \"linear\": The type of learning rate scheduler to use.\n","\n","**seed = 3407**: A random seed for reproducibility.\n","\n","**output_dir = \"outputs\"**: The directory where training outputs will be saved.\n","\n","**report_to = \"none\"**: Disables reporting to external platforms like Weights & Biases."],"metadata":{"id":"6jmBgg9ssmou"}},{"cell_type":"markdown","metadata":{"id":"idAEIeSQ3xdS"},"source":["<a name=\"Train\"></a>\n","### Train the model\n","Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"95_Nn-89DhsL","executionInfo":{"status":"ok","timestamp":1740235693679,"user_tz":0,"elapsed":322113,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["d814bcc16187454686d59cd47297b020","59536e6202404292a13686a1e203dc3a","45f4f598da93491fb232b796cab81a19","6c06255da5ee41208c5f520d3964e5ca","619e00b3125e45779122ce5eecf944c3","34b157f05d06404596eb55a9d1b95c02","4b9033c6621a49c8a7d716392d3b909a","994dfd049c254488af5b1310b176f5c3","3fd7b0a9da244ec2895055ae27000de6","83e1dfbef8ad48dc89081fc8f7313ad0","b78640f2572e46ef8be95f63b0ef3feb","cb4506ba392a410cb95e084fb87bbfd8","4b840e27ad6646e1ab6ec357a9028083","18b7c4c4e54d47c695295c021ac1ef1f","ee1e687fd99b419d9d47ed1376bedaa7","c61e7861c25b417da06039040740ed2c","1de15758be3c40b68184947f6f1fa51a","61a26d971dec4ee396baae0b2c0236bb","5da84bb424e8422aa93b4c96aeeb8e70","cecc3082771f4bb7b724b032a0656f24","26ba682ff6ce4d5399fac8c7d0d483c4","90cbaa1ba54f4a1dbc7ac6afaeecbb09","09e52b878ddf40298aec27701a73e336","b4169ad937344b4aa391065bbf55b354","5a8d86ba1c4c4552acf539007cde13ef","ea2edf09143c44fb80f17d7fbc3acb8e","83fa03e8d92c4ca5a452a309e811c1e8","9e42c5971bf2478db37b6789372e8074","533140334f4049cfbb73fcda73980e69","1f58770a83a043adba253b8bce3bbe3c","fc12afebe85f4b009078155d6cf444a0","4b6ba9f41c5e4f8c8e5ef248aceecc8d","72341f7e2c054caba9017a7c9669bc82","68bc106672894038afae19025371e271","2ff78c600b124e89bb2b0e5f95f29b79","b6f2c961277542dcbe6d85db83b91e68","fff65cdd34c74f028c688e96438e7ec8","7119d6887dda4992bfee5f1cd42a912c","36d26aefaebc4deb84f03b56f651767f","40c4e913f8054bb1a76df2de0c8ed172","a27199fbabf246c39b51df60922f1c85","e8cc544e22d14fed80177b9c0ec92a1d","83bc4bfcc7764913ac2571fc449e4ae7","f8a4abe4f1544cfdb2dc862c5662de91"]},"outputId":"4ea79a5f-682f-4477-dd07-377f47d1ed93"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Converting train dataset to ChatML (num_proc=2):   0%|          | 0/100000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d814bcc16187454686d59cd47297b020"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Applying chat template to train dataset (num_proc=2):   0%|          | 0/100000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb4506ba392a410cb95e084fb87bbfd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Tokenizing train dataset (num_proc=2):   0%|          | 0/100000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09e52b878ddf40298aec27701a73e336"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Tokenizing train dataset (num_proc=2):   0%|          | 0/100000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68bc106672894038afae19025371e271"}},"metadata":{}}],"source":["from trl import SFTTrainer\n","from transformers import TrainingArguments, DataCollatorForSeq2Seq\n","from unsloth import is_bfloat16_supported\n","\n","trainer = SFTTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = dataset,\n","    dataset_text_field = \"text\",\n","    max_seq_length = max_seq_length,\n","    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n","    dataset_num_proc = 2,\n","    packing = False, # Can make training 5x faster for short sequences.\n","    args = TrainingArguments(\n","        per_device_train_batch_size = 2,\n","        gradient_accumulation_steps = 4,\n","        warmup_steps = 5,\n","        # num_train_epochs = 1, # Set this for 1 full training run.\n","        max_steps = 60,\n","        learning_rate = 2e-4,\n","        fp16 = not is_bfloat16_supported(),\n","        bf16 = is_bfloat16_supported(),\n","        logging_steps = 1,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.01,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        output_dir = \"outputs\",\n","        report_to = \"none\", # Use this for WandB etc\n","    ),\n",")"]},{"cell_type":"markdown","source":["This code snippet is designed to modify the training process so that the language model is only trained on the assistant's responses and ignores the loss on the user's inputs. This is a common practice in fine-tuning chatbots, as it helps the model learn to generate better responses rather than trying to predict the user's prompts."],"metadata":{"id":"rHvSZMqwuXgR"}},{"cell_type":"markdown","source":["**from unsloth.chat_templates import train_on_responses_only:** This line imports the train_on_responses_only function from the unsloth.chat_templates module. This function is the key to achieving the desired behavior.\n","\n","**trainer = train_on_responses_only(...):** This line calls the imported train_on_responses_only function and assigns the result back to the trainer variable. This essentially modifies the existing trainer object to incorporate the new training logic.\n","\n","**trainer:** This is the SFTTrainer object that was created earlier and is responsible for managing the training process.\n","\n","**instruction_part** = \"<|start_header_id|>user<|end_header_id|>\\n\\n\": This argument specifies the pattern that identifies the user's instruction or prompt within the training data. It's looking for the specific tokens that mark the beginning and end of the user's turn in the conversation.\n","\n","**response_part** = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\": Similar to the previous argument, this one specifies the pattern that identifies the assistant's response within the training data. It's looking for the tokens that mark the beginning and end of the assistant's turn."],"metadata":{"id":"NKvMCjcfuXoG"}},{"cell_type":"markdown","source":["The train_on_responses_only function takes the trainer object, the patterns for identifying user instructions and assistant responses, and then modifies the training process to mask out the user's input during loss calculation. This ensures that the model is only evaluated and updated based on how well it generates the assistant's responses."],"metadata":{"id":"I4fiJB8suXq7"}},{"cell_type":"markdown","metadata":{"id":"C_sGp5XlG6dq"},"source":["Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"juQiExuBG5Bt","executionInfo":{"status":"ok","timestamp":1740236068626,"user_tz":0,"elapsed":79269,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["b5056fb65fb34a7daa9e919e816f6588","eb0159f541994517bb86a889a80b99fa","057e181d9da54944b543f6c32aa193d0","1fcdfd72cf70496c8c6500f962424655","e5f79c48325e4d7d9c08de48b712ffc8","f263c7be41374a499556a30f7d85da1d","de5efd4e502f41f28fe45322bac19bba","18f81eb3dd2b4616b7060762fe844ae6","8969a8ca03fe40d08f2901aa89e46dd4","fd0064e7466b4816afc7da3f7d5469c1","ec12011fc7a240fc8e100db76e6d4be0"]},"outputId":"76ff59bb-8cd2-4ad1-f74c-ff4cb9b9d0b8"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5056fb65fb34a7daa9e919e816f6588"}},"metadata":{}}],"source":["from unsloth.chat_templates import train_on_responses_only\n","trainer = train_on_responses_only(\n","    trainer,\n","    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n","    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",")"]},{"cell_type":"markdown","source":["This line takes the 5th example from your training dataset, gets the numerical representation of that text, and then uses the tokenizer to translate it back into a format you can understand and read."],"metadata":{"id":"L7-3wUvMu2sJ"}},{"cell_type":"markdown","metadata":{"id":"Dv1NBUozV78l"},"source":["verify masking is actually done:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LtsMVtlkUhja","executionInfo":{"status":"ok","timestamp":1740236100401,"user_tz":0,"elapsed":13,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"colab":{"base_uri":"https://localhost:8080/","height":125},"outputId":"3e9f9871-5230-4591-ffa8-adfadf4a835e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow do astronomers determine the original wavelength of light emitted by a celestial body at rest, which is necessary for measuring its speed using the Doppler effect?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAstronomers make use of the unique spectral fingerprints of elements found in stars. These elements emit and absorb light at specific, known wavelengths, forming an absorption spectrum. By analyzing the light received from distant stars and comparing it to the laboratory-measured spectra of these elements, astronomers can identify the shifts in these wavelengths due to the Doppler effect. The observed shift tells them the extent to which the light has been redshifted or blueshifted, thereby allowing them to calculate the speed of the star along the line of sight relative to Earth.<|eot_id|>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":26}],"source":["tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"]},{"cell_type":"code","source":["trainer.train_dataset[5][\"input_ids\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oIiGytbsI4Lz","executionInfo":{"status":"ok","timestamp":1740236094865,"user_tz":0,"elapsed":40,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"outputId":"20013811-49ae-4543-c0ac-ae4476513f89"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[128000,\n"," 128000,\n"," 128006,\n"," 9125,\n"," 128007,\n"," 271,\n"," 38766,\n"," 1303,\n"," 33025,\n"," 2696,\n"," 25,\n"," 6790,\n"," 220,\n"," 2366,\n"," 18,\n"," 198,\n"," 15724,\n"," 2696,\n"," 25,\n"," 220,\n"," 1627,\n"," 5887,\n"," 220,\n"," 2366,\n"," 19,\n"," 271,\n"," 128009,\n"," 128006,\n"," 882,\n"," 128007,\n"," 271,\n"," 4438,\n"," 656,\n"," 87887,\n"," 8417,\n"," 279,\n"," 4113,\n"," 46406,\n"," 315,\n"," 3177,\n"," 48042,\n"," 555,\n"," 264,\n"," 77480,\n"," 2547,\n"," 520,\n"," 2800,\n"," 11,\n"," 902,\n"," 374,\n"," 5995,\n"," 369,\n"," 30090,\n"," 1202,\n"," 4732,\n"," 1701,\n"," 279,\n"," 97674,\n"," 13206,\n"," 2515,\n"," 30,\n"," 128009,\n"," 128006,\n"," 78191,\n"," 128007,\n"," 271,\n"," 32,\n"," 496,\n"," 14609,\n"," 388,\n"," 1304,\n"," 1005,\n"," 315,\n"," 279,\n"," 5016,\n"," 57077,\n"," 77777,\n"," 315,\n"," 5540,\n"," 1766,\n"," 304,\n"," 9958,\n"," 13,\n"," 4314,\n"," 5540,\n"," 17105,\n"," 323,\n"," 35406,\n"," 3177,\n"," 520,\n"," 3230,\n"," 11,\n"," 3967,\n"," 93959,\n"," 11,\n"," 30164,\n"," 459,\n"," 44225,\n"," 20326,\n"," 13,\n"," 3296,\n"," 42118,\n"," 279,\n"," 3177,\n"," 4036,\n"," 505,\n"," 29827,\n"," 9958,\n"," 323,\n"," 27393,\n"," 433,\n"," 311,\n"," 279,\n"," 27692,\n"," 35073,\n"," 40412,\n"," 63697,\n"," 315,\n"," 1521,\n"," 5540,\n"," 11,\n"," 87887,\n"," 649,\n"," 10765,\n"," 279,\n"," 29735,\n"," 304,\n"," 1521,\n"," 93959,\n"," 4245,\n"," 311,\n"," 279,\n"," 97674,\n"," 13206,\n"," 2515,\n"," 13,\n"," 578,\n"," 13468,\n"," 6541,\n"," 10975,\n"," 1124,\n"," 279,\n"," 13112,\n"," 311,\n"," 902,\n"," 279,\n"," 3177,\n"," 706,\n"," 1027,\n"," 2579,\n"," 13724,\n"," 291,\n"," 477,\n"," 44695,\n"," 48933,\n"," 291,\n"," 11,\n"," 28592,\n"," 10923,\n"," 1124,\n"," 311,\n"," 11294,\n"," 279,\n"," 4732,\n"," 315,\n"," 279,\n"," 6917,\n"," 3235,\n"," 279,\n"," 1584,\n"," 315,\n"," 14254,\n"," 8844,\n"," 311,\n"," 9420,\n"," 13,\n"," 128009]"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["These lines are used to verify that the masking process during training is working correctly. It focuses on how the model is trained to predict only the assistant's responses and ignore the user's prompts. Here's a step-by-step explanation:"],"metadata":{"id":"uhk1y2rfvEhu"}},{"cell_type":"markdown","source":["**space** = tokenizer(\" \", add_special_tokens = False).input_ids[0]:\n","\n","**tokenizer(\" \", add_special_tokens = False)**: This part uses the tokenizer to process a single space character (\" \"). The add_special_tokens = False argument ensures that no special tokens (like beginning-of-sequence or end-of-sequence tokens) are added. The result is a dictionary-like object containing the tokenized representation of the space.\n","\n","**.input_ids[0]**: This extracts the numerical ID of the space token from the tokenized representation. This ID will be used to replace the masked positions.\n","\n","**space** = ...: The result (the numerical ID of the space) is assigned to the variable space.\n","\n","**tokenizer.decode(**[space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]]):\n","\n","**trainer.train_dataset**[5][\"labels\"]: This accesses the \"labels\" of the 5th example in the training dataset. The \"labels\" represent the target sequence the model should learn to predict.\n","\n","[space if x == -100 else x for x in ...]: This is a list comprehension that iterates through each element (x) in the \"labels\" list.\n","\n","If x is equal to -100, it means this position was masked during data preparation. In this case, it's replaced with the space token ID.\n","\n","If x is not -100, it means it's part of the actual target sequence, and it's kept as is.\n","\n","**tokenizer.decode(...)**: This takes the modified list of token IDs and uses the tokenizer to convert them back into human-readable text."],"metadata":{"id":"KZJXy16FvEnk"}},{"cell_type":"markdown","source":[],"metadata":{"id":"f8MD3v4OvEqj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_rD6fl8EUxnG","executionInfo":{"status":"ok","timestamp":1740236146582,"user_tz":0,"elapsed":14,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"colab":{"base_uri":"https://localhost:8080/","height":89},"outputId":"f8a4cba5-4444-4023-eec4-3ad3fb1b2225"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'                                                                 \\n\\nAstronomers make use of the unique spectral fingerprints of elements found in stars. These elements emit and absorb light at specific, known wavelengths, forming an absorption spectrum. By analyzing the light received from distant stars and comparing it to the laboratory-measured spectra of these elements, astronomers can identify the shifts in these wavelengths due to the Doppler effect. The observed shift tells them the extent to which the light has been redshifted or blueshifted, thereby allowing them to calculate the speed of the star along the line of sight relative to Earth.<|eot_id|>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":27}],"source":["space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n","tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"]},{"cell_type":"markdown","metadata":{"id":"3enWUM0jV-jV"},"source":["We can see the System and Instruction prompts are successfully masked!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ejIt2xSNKKp","executionInfo":{"status":"ok","timestamp":1740236207003,"user_tz":0,"elapsed":4,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"897e7728-9c51-4c03-915f-350633ce28a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["GPU = Tesla T4. Max memory = 14.741 GB.\n","4.51 GB of memory reserved.\n"]}],"source":["# @title Show current memory stats\n","gpu_stats = torch.cuda.get_device_properties(0)\n","start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n","print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n","print(f\"{start_gpu_memory} GB of memory reserved.\")"]},{"cell_type":"markdown","source":["This single line of code is where the fine-tuning process actually begins.\n","\n","**trainer**: This is the SFTTrainer object that you previously configured with all the settings for training (dataset, model, hyperparameters, etc.). It's essentially the manager of the training process.\n","\n","**.train()**: This is a method of the SFTTrainer class that starts the training loop. The model will begin learning from the data you provided. It iterates over the dataset, calculates the loss, updates the model's weights, and repeats this process for the specified number of steps or epochs.\n","\n","**trainer_stats**= ...: The trainer.train() method returns information about the training process, such as the training loss, time taken, and other metrics. These stats are assigned to the variable trainer_stats so you can access and analyze them later if needed."],"metadata":{"id":"ztrNvaMPvlQF"}},{"cell_type":"markdown","source":["Imagine trainer as a coach, .train() as the command to start the training session, and trainer_stats as the report card after the session. This line initiates the actual learning phase for your"],"metadata":{"id":"uTyPAmUwvtDC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yqxqAZ7KJ4oL","executionInfo":{"status":"ok","timestamp":1740236706100,"user_tz":0,"elapsed":485709,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"449ad87a-420e-4418-d122-c5627d0ec0ef"},"outputs":[{"output_type":"stream","name":"stderr","text":["==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n","   \\\\   /|    Num examples = 100,000 | Num Epochs = 1\n","O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n","\\        /    Total batch size = 8 | Total steps = 60\n"," \"-____-\"     Number of trainable parameters = 24,313,856\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [60/60 07:39, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.772600</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.836100</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.070600</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.888800</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.755000</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.932600</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.617400</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.993300</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.858000</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.759200</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.882100</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>1.090500</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.950100</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.639500</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.875100</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.637200</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.999000</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>0.824500</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>0.767300</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.929700</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>0.899100</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>0.854200</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>1.033100</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>0.881400</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.639900</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>0.825300</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>0.825100</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>0.784900</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>1.082800</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.033200</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>0.705700</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>0.539900</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>0.652800</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>0.578400</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.759900</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>0.999000</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>0.897700</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>0.714800</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>0.777100</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.996000</td>\n","    </tr>\n","    <tr>\n","      <td>41</td>\n","      <td>0.743300</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>1.005300</td>\n","    </tr>\n","    <tr>\n","      <td>43</td>\n","      <td>0.769400</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>0.812500</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.760600</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>0.860600</td>\n","    </tr>\n","    <tr>\n","      <td>47</td>\n","      <td>0.784900</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>0.650500</td>\n","    </tr>\n","    <tr>\n","      <td>49</td>\n","      <td>1.012200</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>1.028800</td>\n","    </tr>\n","    <tr>\n","      <td>51</td>\n","      <td>0.455700</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>0.904500</td>\n","    </tr>\n","    <tr>\n","      <td>53</td>\n","      <td>1.311400</td>\n","    </tr>\n","    <tr>\n","      <td>54</td>\n","      <td>0.689000</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>1.057400</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>1.121600</td>\n","    </tr>\n","    <tr>\n","      <td>57</td>\n","      <td>0.723800</td>\n","    </tr>\n","    <tr>\n","      <td>58</td>\n","      <td>0.833500</td>\n","    </tr>\n","    <tr>\n","      <td>59</td>\n","      <td>0.753400</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.921200</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}}],"source":["trainer_stats = trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pCqnaKmlO1U9","executionInfo":{"status":"ok","timestamp":1740238800686,"user_tz":0,"elapsed":6,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2c9d61fb-f7f3-4d62-ddc8-6e5f0a3852c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["483.0789 seconds used for training.\n","8.05 minutes used for training.\n","Peak reserved memory = 4.51 GB.\n","Peak reserved memory for training = 0.0 GB.\n","Peak reserved memory % of max memory = 30.595 %.\n","Peak reserved memory for training % of max memory = 0.0 %.\n"]}],"source":["# @title Show final memory and time stats\n","used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n","used_percentage = round(used_memory / max_memory * 100, 3)\n","lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n","print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n","print(\n","    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",")\n","print(f\"Peak reserved memory = {used_memory} GB.\")\n","print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n","print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n","print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"]},{"cell_type":"markdown","source":["**from unsloth.chat_templates import get_chat_template**: This line imports a function called get_chat_template which is used to format the prompts for the model.\n","\n","**tokenizer **= get_chat_template(...): This applies the \"llama-3.1\" chat template to your tokenizer. This template defines how the model expects the input to be structured, following the style of the Llama 3.1 model.\n","\n","**FastLanguageModel.for_inference(model)**: This line optimizes the model specifically for inference, potentially making the prediction process faster."],"metadata":{"id":"-IXOSMSgv3-m"}},{"cell_type":"markdown","source":["**Providing Input to the Model**\n"],"metadata":{"id":"l_K7JgG0v4Dh"}},{"cell_type":"markdown","source":["**messages** = [...]: This defines the input to the model. In this case, it's a list containing a single message, simulating a user asking the model to continue the Fibonacci sequence.\n","\n","**inputs** = tokenizer.apply_chat_template(...): This takes the messages, applies the chat template, tokenizes them (converts them to numerical representations), adds a special prompt to signal the start of generation, and formats the output as PyTorch tensors. Finally, it moves the tensors to the \"cuda\" device, indicating it will be processed on a GPU if available."],"metadata":{"id":"NtXFbiD6v4Gr"}},{"cell_type":"markdown","source":["**Generating Output**\n"],"metadata":{"id":"WBddleHsv4Jb"}},{"cell_type":"markdown","source":["**outputs**= model.generate(...): This is the core of inference. It uses the model to generate a response based on the provided inputs.\n","\n","**max_new_tokens** = 64: Limits the generated output to a maximum of 64 tokens.\n","\n","**use_cache** = True: Enables caching to speed up the generation process.\n","\n","**temperature** = 1.5, min_p = 0.1: These parameters control the randomness and creativity of the generated text.\n","\n","**tokenizer.batch_decode(outputs)**: Finally, this line takes the numerical output (outputs) from the model and decodes it back into human-readable text using the tokenizer. This is the final prediction that the model produces."],"metadata":{"id":"H6N2A6M8v4MH"}},{"cell_type":"markdown","metadata":{"id":"ekOmTR1hSNcr"},"source":["<a name=\"Inference\"></a>\n","### Inference\n","Let's run the model! You can change the instruction and input - leave the output blank!\n","\n","**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**\n","\n","We use `min_p = 0.1` and `temperature = 1.5`. Read this [Tweet](https://x.com/menhguin/status/1826132708508213629) for more information on why."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kR3gIAX-SM2q","executionInfo":{"status":"ok","timestamp":1740239036395,"user_tz":0,"elapsed":4584,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e1b8d329-08c3-435a-d5b3-9921c5e92f71"},"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"]},{"output_type":"execute_result","data":{"text/plain":["['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nContinue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe Fibonacci sequence is a series of numbers in which each number is the sum of the two preceding numbers. The sequence you provided starts with 1, 1, 2, 3, 5, and 8. Here are the next three numbers in the sequence:\\n9, 14, 23<|eot_id|>']"]},"metadata":{},"execution_count":31}],"source":["from unsloth.chat_templates import get_chat_template\n","\n","tokenizer = get_chat_template(\n","    tokenizer,\n","    chat_template = \"llama-3.1\",\n",")\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","\n","messages = [\n","    {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n","]\n","inputs = tokenizer.apply_chat_template(\n","    messages,\n","    tokenize = True,\n","    add_generation_prompt = True, # Must add for generation\n","    return_tensors = \"pt\",\n",").to(\"cuda\")\n","\n","outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n","                         temperature = 1.5, min_p = 0.1)\n","tokenizer.batch_decode(outputs)"]},{"cell_type":"markdown","source":["**Inference with Text Streaming**\n"],"metadata":{"id":"OIAJbI_ewbT8"}},{"cell_type":"markdown","source":["This code snippet focuses on using the TextStreamer from the transformers library to see the model's output as it's generated, token by token, rather than waiting for the entire generation to complete."],"metadata":{"id":"qJKiJZKFwbXs"}},{"cell_type":"markdown","source":["**Optimizing for Inference**\n"],"metadata":{"id":"07TCVdA-wba1"}},{"cell_type":"markdown","source":["**FastLanguageModel.for_inference(model)**: This line is specific to the Unsloth library. It likely optimizes the model for inference, potentially making predictions faster."],"metadata":{"id":"nUGnT318wbd1"}},{"cell_type":"markdown","source":["**messages**: This defines the input to the model, structured as a list of dictionaries. Each dictionary represents a turn in the conversation, with \"role\" indicating who is speaking (user or assistant) and \"content\" containing the message.\n","\n","**inputs**= tokenizer.apply_chat_template(...): This line processes the messages using the tokenizer:\n","\n","**apply_chat_template**: Formats the messages according to the specific chat template being used (likely Llama 3.1 in this case).\n","\n","**tokenize** = True: Tokenizes the text, converting it into numerical representations the model understands.\n","\n","**add_generation_prompt** = True: Adds a special prompt to signal the model to start generating text.\n","\n","**return_tensors** = \"pt\": Returns the output as PyTorch tensors.\n",".to(\"cuda\"): Moves the tensors to the \"cuda\" device (GPU) if available for faster processing."],"metadata":{"id":"gUNkvkeDwwVI"}},{"cell_type":"markdown","source":["**from transformers import TextStreamer**: Imports the TextStreamer class.\n","\n","**text_streamer** = TextStreamer(tokenizer, skip_prompt = True): Creates a TextStreamer instance. skip_prompt = True likely means the initial prompt won't be printed in the output stream.\n","\n","**_ = model.generate(...)**: This uses the model to generate text, but instead of waiting for the entire generation, it streams the output:\n","\n","**input_ids** = inputs: Provides the tokenized input.\n","\n","**streamer** = text_streamer: Uses the text_streamer to handle the output stream.\n","\n","**max_new_tokens** = 128: Limits the generated output to a maximum of 128 tokens.\n","\n","**use_cache** = True: Enables caching for faster generation.\n","\n","**temperature** = 1.5, min_p = 0.1: These parameters influence the randomness and creativity of the generated text."],"metadata":{"id":"COkmg7b-wbgh"}},{"cell_type":"markdown","metadata":{"id":"CrSvZObor0lY"},"source":["use a`TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e2pEuRb1r2Vg","executionInfo":{"status":"ok","timestamp":1740239138644,"user_tz":0,"elapsed":2534,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b070592e-edb0-4e34-e222-bf632e2c18c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Certainly! Here is the next few terms in the Fibonacci sequence: 13, 21, 34, 55, 89, 144, 233. Let me know if you need anything else!<|eot_id|>\n"]}],"source":["FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","\n","messages = [\n","    {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n","]\n","inputs = tokenizer.apply_chat_template(\n","    messages,\n","    tokenize = True,\n","    add_generation_prompt = True, # Must add for generation\n","    return_tensors = \"pt\",\n",").to(\"cuda\")\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n","_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n","                   use_cache = True, temperature = 1.5, min_p = 0.1)"]},{"cell_type":"markdown","metadata":{"id":"uMuVrWbjAzhc"},"source":["<a name=\"Save\"></a>\n","### Saving, loading trained/finetuned models\n","To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n","\n","**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"upcOlWe7A1vc","executionInfo":{"status":"ok","timestamp":1740239172265,"user_tz":0,"elapsed":1948,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"384ad034-36bd-416c-ab01-da58540d81ea"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["('lora_model_rahulraj/tokenizer_config.json',\n"," 'lora_model_rahulraj/special_tokens_map.json',\n"," 'lora_model_rahulraj/tokenizer.json')"]},"metadata":{},"execution_count":36}],"source":["model.save_pretrained(\"lora_model_rahulraj\")  # Local saving\n","tokenizer.save_pretrained(\"lora_model_rahulraj\")\n","# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n","# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"]},{"cell_type":"markdown","metadata":{"id":"AEEcJ4qfC7Lp"},"source":["Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MKX_XKs_BNZR","executionInfo":{"status":"ok","timestamp":1740239513612,"user_tz":0,"elapsed":27967,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"400a93ab-600c-4f99-f5c8-d87af23fd526"},"outputs":[{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.48.3.\n","   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29. FA2 = False]\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","The Eiffel Tower in Paris, the capital of France, is an iconic tower that stands at over 1,000 feet (300 meters) high. It was built for the 1889 World's Fair to celebrate the 100th anniversary of the French Revolution. The tower is supported by four pillars located at its base, and its latticework structure allows it to flex and resist strong winds.\n","\n","The Eiffel Tower was the tallest man-made structure in the world for nearly 40 years and has become a symbol of Paris and the French Republic. The tower has been repurposed for various uses, including as a\n"]}],"source":["if True:\n","    from unsloth import FastLanguageModel\n","    model, tokenizer = FastLanguageModel.from_pretrained(\n","        model_name = \"lora_model_rahulraj\", # YOUR MODEL YOU USED FOR TRAINING\n","        max_seq_length = max_seq_length,\n","        dtype = dtype,\n","        load_in_4bit = load_in_4bit,\n","    )\n","    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","\n","messages = [\n","    {\"role\": \"user\", \"content\": \"Describe a tall tower in the capital of France.\"},\n","]\n","inputs = tokenizer.apply_chat_template(\n","    messages,\n","    tokenize = True,\n","    add_generation_prompt = True, # Must add for generation\n","    return_tensors = \"pt\",\n",").to(\"cuda\")\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n","_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n","                   use_cache = True, temperature = 1.5, min_p = 0.1)"]},{"cell_type":"markdown","source":["**model.push_to_hub_merged(...)**: This is the function call that initiates the saving and uploading process. It's a method provided by the Unsloth library (or a library it depends on) that's specifically designed for this purpose.\n","\n","**\"mynamerahulkumar/model\"**: This is the repository name on the Hugging Face Model Hub where your model will be saved. It's in the format \"username/model_name\". In this case, it's saving to a repository named \"model\" under the user \"mynamerahulkumar\".\n","\n","**tokenizer**: This argument passes the tokenizer associated with your model. The tokenizer is crucial for converting text into numerical representations that the model understands. When you load the model later, you'll also need the tokenizer to use it correctly.\n","\n","**save_method** = \"merged_16bit\": This argument specifies how the model should be saved. \"merged_16bit\" indicates that the model and LoRA adapters will be merged and saved in 16-bit floating-point precision. This can reduce the file size of the model while maintaining reasonable accuracy.\n","\n","**token** = \"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\": This is your Hugging Face user access token. It's needed to authenticate and authorize the upload to your Hugging Face account. You can find or generate your token in the settings of your Hugging Face account."],"metadata":{"id":"LMW3XFrvxumn"}},{"cell_type":"markdown","source":[],"metadata":{"id":"PRpJqB5exuqy"}},{"cell_type":"markdown","metadata":{"id":"f422JgM9sdVT"},"source":["### Saving to float16 for VLLM\n","\n","We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."]},{"cell_type":"code","source":["model.push_to_hub_merged(\"mynamerahulkumar/model\", tokenizer, save_method = \"merged_16bit\", token = \"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\")"],"metadata":{"id":"8Kml_CevnPz6","executionInfo":{"status":"ok","timestamp":1740239753083,"user_tz":0,"elapsed":181088,"user":{"displayName":"RAHUL KUMAR","userId":"05449709764039105982"}},"colab":{"base_uri":"https://localhost:8080/","height":484,"referenced_widgets":["dbdfe6f4bf1a4140802c947b994ba4ec","ecbd66bed5fc4fc8ae65aff5a1c8ca07","7f1f0eaae0524204a65cf106531ed862","f5096ba78643476e821b259060410f4f","b20df08bc7854e0184b51efd3b8b7114","9061137b2e2e487fb4ef002629d96265","038543ed5fbb4960ade8bfbbb47aa4d8","c05d1a94434c4a129792e6df4bb55192","da9f558af0d6451c9979ea067ff93430","7d03a18b1eee47a987cb387232027907","c85cd52298944ea3b88ac68b2f60dbf5","843fa7f7051c46be81147d6332db320e","76f6dee7fb9e4e9ba8ea0264940b113d","5112a586f07346ffba37517e4aeeea6a","7fcbbe058e3e455aac2a2e355546977e","63f28b1780004f95b160b65cc5986ed4","47f145ad4fd54f6f8750363186489aff","1b214428e98947d59ac1c7806de47514","e031ec4ad59548f19cd97eafff19acb0","33d3785a30b440c380d1f299824cba9e","4768957a180a4aa3acf67ec6c3ba4170","02059710e7fc4a228977b2c8c314f514","8df6529f754e4297936d847e6a21c907","ece3a5d9d5dc4febb9ce8094a72568a2","aacf177ef74f4f30bee4667f47622390","ac74b02a8f374dfdb5e1549a1d9d1d2f","83c71752ca81498f8bfffe0922654b01","b8eb6404c1c84e2dbb8eefc15112df4a","db40f0a1a54849e8839c3f871b21a75c","a7fee32934c14741880405b334da6f9a","e29b1454045f401486c49e2583e5f971","a3425c0ac2ef411a92c3ef305a9b88be","133c5c2e8ddd419fb1e578076e667eb9","8935c7faa0b54e5e96b52376ba04d460","6211fea0f5cb48fc8943293a19d9dad4","1c0b5a7302bd4f6db5bf6158a648cb2e","5e34d45ca99b4131903ebadbe844a025","d1e9407d67bc43f39eeb4df751e77100","f849d367fa1e4151a9dbc2e3abe37baf","884d9c88bfec400db807f6360a37d302","22d5bd8c230b407ba5b0c977a1ec8fa7","1a93cd576cf94574afdb30901f2fd2a3","2b1b81d09c0d430186625c7f7051ad2c","f159e6f0d2064510b3a04158bdc70023"]},"outputId":"2bb001c5-3b74-48db-f25d-d11d0872d079"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Unsloth: You are pushing to hub, but you passed your HF username = mynamerahulkumar.\n","We shall truncate mynamerahulkumar/model to model\n","Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n","We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\n","To force `safe_serialization`, set it to `None` instead.\n","Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n","model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n","Unsloth: Will remove a cached repo with size 2.4G\n"]},{"output_type":"stream","name":"stdout","text":["Unsloth: Merging 4bit and LoRA weights to 16bit...\n","Unsloth: Will use up to 3.63 out of 12.67 RAM for saving.\n","Unsloth: Saving model... This might take 5 minutes ...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [00:01<00:00, 22.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Unsloth: Saving tokenizer..."]},{"output_type":"stream","name":"stderr","text":["No files have been modified since last commit. Skipping to prevent empty commit.\n","WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"]},{"output_type":"stream","name":"stdout","text":[" Done.\n","Unsloth: Saving model/pytorch_model-00001-of-00002.bin...\n","Unsloth: Saving model/pytorch_model-00002-of-00002.bin...\n"]},{"output_type":"display_data","data":{"text/plain":["README.md:   0%|          | 0.00/627 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbdfe6f4bf1a4140802c947b994ba4ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"843fa7f7051c46be81147d6332db320e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model-00002-of-00002.bin:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8df6529f754e4297936d847e6a21c907"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model-00001-of-00002.bin:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8935c7faa0b54e5e96b52376ba04d460"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Done.\n","Saved merged model to https://huggingface.co/mynamerahulkumar/model\n"]}]},{"cell_type":"markdown","source":["This line of code takes your trained model (model), including the learned adjustments (LoRA adapters), and saves it into a folder named \"model\". It also saves the tokenizer which is necessary to understand the model's input and output. The model is saved in a compact format (merged_16bit) to reduce its size.\n","\n"],"metadata":{"id":"SpEQTDjix_OQ"}},{"cell_type":"code","source":["# Merge to 16bit\n","model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n"],"metadata":{"id":"TDA7xLu8n4FJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**save_method** = \"merged_4bit\": This argument specifies how the model should be saved:\n","\n","**merged_4bit**: This means the model and its LoRA adapters (the learned changes during fine-tuning) will be merged together and saved using 4-bit quantization.\n","\n","Merging makes it easier to load and use the model later.\n","\n","**4-bit Quantization** is a technique to reduce the model's size by storing its weights using fewer bits, making it more efficient for storage and download."],"metadata":{"id":"qSe9YNR8yNXF"}},{"cell_type":"code","source":["\n","# Merge to 4bit\n","model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n"],"metadata":{"id":"wLgKODxhn8hY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.push_to_hub_merged(\"mynamerahulkumar/model\", tokenizer, save_method = \"merged_4bit\", token = \"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\")\n"],"metadata":{"id":"FEYlvd0Boz21"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**save_method = \"lora\"**: This argument specifies how the model should be saved. \"lora\" indicates that only the LoRA adapters (the changes made during fine-tuning) will be saved, and not the entire base model. This is often preferred as it results in much smaller file sizes compared to saving the full model"],"metadata":{"id":"2MzOK5JPyb82"}},{"cell_type":"code","source":["\n","# Just LoRA adapters\n","model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n"],"metadata":{"id":"UbF8evvsn8o3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.push_to_hub_merged(\"mynamerahulkumar/model\", tokenizer, save_method = \"lora\", token = \"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\")"],"metadata":{"id":"5p4vig41o05k"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iHjt_SMYsd3P"},"outputs":[],"source":["# # Merge to 16bit\n","# if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n","# if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"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\")\n","\n","# # Merge to 4bit\n","# if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n","# if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"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\")\n","\n","# # Just LoRA adapters\n","# if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n","# if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"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\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"d814bcc16187454686d59cd47297b020":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_59536e6202404292a13686a1e203dc3a","IPY_MODEL_45f4f598da93491fb232b796cab81a19","IPY_MODEL_6c06255da5ee41208c5f520d3964e5ca"],"layout":"IPY_MODEL_619e00b3125e45779122ce5eecf944c3"}},"59536e6202404292a13686a1e203dc3a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_34b157f05d06404596eb55a9d1b95c02","placeholder":"​","style":"IPY_MODEL_4b9033c6621a49c8a7d716392d3b909a","value":"Converting train dataset to ChatML (num_proc=2): 100%"}},"45f4f598da93491fb232b796cab81a19":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_994dfd049c254488af5b1310b176f5c3","max":100000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3fd7b0a9da244ec2895055ae27000de6","value":100000}},"6c06255da5ee41208c5f520d3964e5ca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_83e1dfbef8ad48dc89081fc8f7313ad0","placeholder":"​","style":"IPY_MODEL_b78640f2572e46ef8be95f63b0ef3feb","value":" 100000/100000 [00:14&lt;00:00, 8935.54 examples/s]"}},"619e00b3125e45779122ce5eecf944c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34b157f05d06404596eb55a9d1b95c02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b9033c6621a49c8a7d716392d3b909a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"994dfd049c254488af5b1310b176f5c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fd7b0a9da244ec2895055ae27000de6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"83e1dfbef8ad48dc89081fc8f7313ad0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b78640f2572e46ef8be95f63b0ef3feb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb4506ba392a410cb95e084fb87bbfd8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4b840e27ad6646e1ab6ec357a9028083","IPY_MODEL_18b7c4c4e54d47c695295c021ac1ef1f","IPY_MODEL_ee1e687fd99b419d9d47ed1376bedaa7"],"layout":"IPY_MODEL_c61e7861c25b417da06039040740ed2c"}},"4b840e27ad6646e1ab6ec357a9028083":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1de15758be3c40b68184947f6f1fa51a","placeholder":"​","style":"IPY_MODEL_61a26d971dec4ee396baae0b2c0236bb","value":"Applying chat template to train dataset (num_proc=2): 100%"}},"18b7c4c4e54d47c695295c021ac1ef1f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5da84bb424e8422aa93b4c96aeeb8e70","max":100000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cecc3082771f4bb7b724b032a0656f24","value":100000}},"ee1e687fd99b419d9d47ed1376bedaa7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_26ba682ff6ce4d5399fac8c7d0d483c4","placeholder":"​","style":"IPY_MODEL_90cbaa1ba54f4a1dbc7ac6afaeecbb09","value":" 100000/100000 [00:21&lt;00:00, 4927.48 examples/s]"}},"c61e7861c25b417da06039040740ed2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1de15758be3c40b68184947f6f1fa51a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61a26d971dec4ee396baae0b2c0236bb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5da84bb424e8422aa93b4c96aeeb8e70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cecc3082771f4bb7b724b032a0656f24":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"26ba682ff6ce4d5399fac8c7d0d483c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90cbaa1ba54f4a1dbc7ac6afaeecbb09":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"09e52b878ddf40298aec27701a73e336":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b4169ad937344b4aa391065bbf55b354","IPY_MODEL_5a8d86ba1c4c4552acf539007cde13ef","IPY_MODEL_ea2edf09143c44fb80f17d7fbc3acb8e"],"layout":"IPY_MODEL_83fa03e8d92c4ca5a452a309e811c1e8"}},"b4169ad937344b4aa391065bbf55b354":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e42c5971bf2478db37b6789372e8074","placeholder":"​","style":"IPY_MODEL_533140334f4049cfbb73fcda73980e69","value":"Tokenizing train dataset (num_proc=2): 100%"}},"5a8d86ba1c4c4552acf539007cde13ef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f58770a83a043adba253b8bce3bbe3c","max":100000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fc12afebe85f4b009078155d6cf444a0","value":100000}},"ea2edf09143c44fb80f17d7fbc3acb8e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b6ba9f41c5e4f8c8e5ef248aceecc8d","placeholder":"​","style":"IPY_MODEL_72341f7e2c054caba9017a7c9669bc82","value":" 100000/100000 [03:18&lt;00:00, 340.07 examples/s]"}},"83fa03e8d92c4ca5a452a309e811c1e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e42c5971bf2478db37b6789372e8074":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"533140334f4049cfbb73fcda73980e69":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f58770a83a043adba253b8bce3bbe3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc12afebe85f4b009078155d6cf444a0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4b6ba9f41c5e4f8c8e5ef248aceecc8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72341f7e2c054caba9017a7c9669bc82":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"68bc106672894038afae19025371e271":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2ff78c600b124e89bb2b0e5f95f29b79","IPY_MODEL_b6f2c961277542dcbe6d85db83b91e68","IPY_MODEL_fff65cdd34c74f028c688e96438e7ec8"],"layout":"IPY_MODEL_7119d6887dda4992bfee5f1cd42a912c"}},"2ff78c600b124e89bb2b0e5f95f29b79":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_36d26aefaebc4deb84f03b56f651767f","placeholder":"​","style":"IPY_MODEL_40c4e913f8054bb1a76df2de0c8ed172","value":"Tokenizing train dataset (num_proc=2): 100%"}},"b6f2c961277542dcbe6d85db83b91e68":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a27199fbabf246c39b51df60922f1c85","max":100000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e8cc544e22d14fed80177b9c0ec92a1d","value":100000}},"fff65cdd34c74f028c688e96438e7ec8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_83bc4bfcc7764913ac2571fc449e4ae7","placeholder":"​","style":"IPY_MODEL_f8a4abe4f1544cfdb2dc862c5662de91","value":" 100000/100000 [01:26&lt;00:00, 1038.22 examples/s]"}},"7119d6887dda4992bfee5f1cd42a912c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36d26aefaebc4deb84f03b56f651767f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40c4e913f8054bb1a76df2de0c8ed172":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a27199fbabf246c39b51df60922f1c85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8cc544e22d14fed80177b9c0ec92a1d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"83bc4bfcc7764913ac2571fc449e4ae7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8a4abe4f1544cfdb2dc862c5662de91":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5056fb65fb34a7daa9e919e816f6588":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eb0159f541994517bb86a889a80b99fa","IPY_MODEL_057e181d9da54944b543f6c32aa193d0","IPY_MODEL_1fcdfd72cf70496c8c6500f962424655"],"layout":"IPY_MODEL_e5f79c48325e4d7d9c08de48b712ffc8"}},"eb0159f541994517bb86a889a80b99fa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f263c7be41374a499556a30f7d85da1d","placeholder":"​","style":"IPY_MODEL_de5efd4e502f41f28fe45322bac19bba","value":"Map: 100%"}},"057e181d9da54944b543f6c32aa193d0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_18f81eb3dd2b4616b7060762fe844ae6","max":100000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8969a8ca03fe40d08f2901aa89e46dd4","value":100000}},"1fcdfd72cf70496c8c6500f962424655":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd0064e7466b4816afc7da3f7d5469c1","placeholder":"​","style":"IPY_MODEL_ec12011fc7a240fc8e100db76e6d4be0","value":" 100000/100000 [01:19&lt;00:00, 1855.01 examples/s]"}},"e5f79c48325e4d7d9c08de48b712ffc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f263c7be41374a499556a30f7d85da1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de5efd4e502f41f28fe45322bac19bba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18f81eb3dd2b4616b7060762fe844ae6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8969a8ca03fe40d08f2901aa89e46dd4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fd0064e7466b4816afc7da3f7d5469c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec12011fc7a240fc8e100db76e6d4be0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dbdfe6f4bf1a4140802c947b994ba4ec":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ecbd66bed5fc4fc8ae65aff5a1c8ca07","IPY_MODEL_7f1f0eaae0524204a65cf106531ed862","IPY_MODEL_f5096ba78643476e821b259060410f4f"],"layout":"IPY_MODEL_b20df08bc7854e0184b51efd3b8b7114"}},"ecbd66bed5fc4fc8ae65aff5a1c8ca07":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9061137b2e2e487fb4ef002629d96265","placeholder":"​","style":"IPY_MODEL_038543ed5fbb4960ade8bfbbb47aa4d8","value":"README.md: 100%"}},"7f1f0eaae0524204a65cf106531ed862":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c05d1a94434c4a129792e6df4bb55192","max":627,"min":0,"orientation":"horizontal","style":"IPY_MODEL_da9f558af0d6451c9979ea067ff93430","value":627}},"f5096ba78643476e821b259060410f4f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d03a18b1eee47a987cb387232027907","placeholder":"​","style":"IPY_MODEL_c85cd52298944ea3b88ac68b2f60dbf5","value":" 627/627 [00:00&lt;00:00, 53.0kB/s]"}},"b20df08bc7854e0184b51efd3b8b7114":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9061137b2e2e487fb4ef002629d96265":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"038543ed5fbb4960ade8bfbbb47aa4d8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c05d1a94434c4a129792e6df4bb55192":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da9f558af0d6451c9979ea067ff93430":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7d03a18b1eee47a987cb387232027907":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c85cd52298944ea3b88ac68b2f60dbf5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"843fa7f7051c46be81147d6332db320e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_76f6dee7fb9e4e9ba8ea0264940b113d","IPY_MODEL_5112a586f07346ffba37517e4aeeea6a","IPY_MODEL_7fcbbe058e3e455aac2a2e355546977e"],"layout":"IPY_MODEL_63f28b1780004f95b160b65cc5986ed4"}},"76f6dee7fb9e4e9ba8ea0264940b113d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_47f145ad4fd54f6f8750363186489aff","placeholder":"​","style":"IPY_MODEL_1b214428e98947d59ac1c7806de47514","value":"100%"}},"5112a586f07346ffba37517e4aeeea6a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e031ec4ad59548f19cd97eafff19acb0","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_33d3785a30b440c380d1f299824cba9e","value":2}},"7fcbbe058e3e455aac2a2e355546977e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4768957a180a4aa3acf67ec6c3ba4170","placeholder":"​","style":"IPY_MODEL_02059710e7fc4a228977b2c8c314f514","value":" 2/2 [01:01&lt;00:00, 33.38s/it]"}},"63f28b1780004f95b160b65cc5986ed4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47f145ad4fd54f6f8750363186489aff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b214428e98947d59ac1c7806de47514":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e031ec4ad59548f19cd97eafff19acb0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33d3785a30b440c380d1f299824cba9e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4768957a180a4aa3acf67ec6c3ba4170":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02059710e7fc4a228977b2c8c314f514":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8df6529f754e4297936d847e6a21c907":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ece3a5d9d5dc4febb9ce8094a72568a2","IPY_MODEL_aacf177ef74f4f30bee4667f47622390","IPY_MODEL_ac74b02a8f374dfdb5e1549a1d9d1d2f"],"layout":"IPY_MODEL_83c71752ca81498f8bfffe0922654b01"}},"ece3a5d9d5dc4febb9ce8094a72568a2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8eb6404c1c84e2dbb8eefc15112df4a","placeholder":"​","style":"IPY_MODEL_db40f0a1a54849e8839c3f871b21a75c","value":"pytorch_model-00002-of-00002.bin: "}},"aacf177ef74f4f30bee4667f47622390":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a7fee32934c14741880405b334da6f9a","max":1459746080,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e29b1454045f401486c49e2583e5f971","value":1459746080}},"ac74b02a8f374dfdb5e1549a1d9d1d2f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3425c0ac2ef411a92c3ef305a9b88be","placeholder":"​","style":"IPY_MODEL_133c5c2e8ddd419fb1e578076e667eb9","value":" 1.47G/? [00:14&lt;00:00, 171MB/s]"}},"83c71752ca81498f8bfffe0922654b01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8eb6404c1c84e2dbb8eefc15112df4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db40f0a1a54849e8839c3f871b21a75c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a7fee32934c14741880405b334da6f9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e29b1454045f401486c49e2583e5f971":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a3425c0ac2ef411a92c3ef305a9b88be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"133c5c2e8ddd419fb1e578076e667eb9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8935c7faa0b54e5e96b52376ba04d460":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6211fea0f5cb48fc8943293a19d9dad4","IPY_MODEL_1c0b5a7302bd4f6db5bf6158a648cb2e","IPY_MODEL_5e34d45ca99b4131903ebadbe844a025"],"layout":"IPY_MODEL_d1e9407d67bc43f39eeb4df751e77100"}},"6211fea0f5cb48fc8943293a19d9dad4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f849d367fa1e4151a9dbc2e3abe37baf","placeholder":"​","style":"IPY_MODEL_884d9c88bfec400db807f6360a37d302","value":"pytorch_model-00001-of-00002.bin: "}},"1c0b5a7302bd4f6db5bf6158a648cb2e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_22d5bd8c230b407ba5b0c977a1ec8fa7","max":4965844039,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1a93cd576cf94574afdb30901f2fd2a3","value":4965844039}},"5e34d45ca99b4131903ebadbe844a025":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b1b81d09c0d430186625c7f7051ad2c","placeholder":"​","style":"IPY_MODEL_f159e6f0d2064510b3a04158bdc70023","value":" 4.98G/? [00:45&lt;00:00, 258MB/s]"}},"d1e9407d67bc43f39eeb4df751e77100":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f849d367fa1e4151a9dbc2e3abe37baf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"884d9c88bfec400db807f6360a37d302":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"22d5bd8c230b407ba5b0c977a1ec8fa7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a93cd576cf94574afdb30901f2fd2a3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2b1b81d09c0d430186625c7f7051ad2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f159e6f0d2064510b3a04158bdc70023":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}