{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjLq3hMOP1KR"
   },
   "source": [
    "### Evaluating RAG: Using LLM as a Judge (With Structured Outputs)\n",
    "\n",
    "This cookbook shows an example of using the Mistral AI models for LLM As A Judge using structured outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vB4dHYDU9mST",
    "outputId": "e6ef92d6-778f-4ee5-9783-5ee1d6078a0f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "from enum import Enum\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = \"05.5_output.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For variation use a function to get the LLM client based on the user's choice\n",
    "\n",
    "\n",
    "def get_llm_client(llm_choice):\n",
    "\n",
    "    if llm_choice == \"GROQ\":\n",
    "\n",
    "        client = OpenAI(\n",
    "            base_url=\"https://api.groq.com/openai/v1\",\n",
    "            api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "        )\n",
    "\n",
    "        return client\n",
    "\n",
    "    elif llm_choice == \"OPENAI\":\n",
    "\n",
    "        load_dotenv()  # load environment variables from .env fil\n",
    "\n",
    "        client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "        return client\n",
    "\n",
    "    else:\n",
    "\n",
    "        raise ValueError(\"Invalid LLM choice. Please choose 'GROQ' or 'OPENAI'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY exists and begins sk-proj-vIt-L1...\n",
      "GROQ_API_KEY exists and begins gsk_0yKDCuUXkz...\n",
      "LLM_CHOICE: OPENAI - MODEL: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "LLM_CHOICE = \"OPENAI\"\n",
    "# LLM_CHOICE = \"GROQ\" # GROWQ uses different API\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    print(f\"OPENAI_API_KEY exists and begins {OPENAI_API_KEY[:14]}...\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY not set\")\n",
    "\n",
    "if GROQ_API_KEY:\n",
    "    print(f\"GROQ_API_KEY exists and begins {GROQ_API_KEY[:14]}...\")\n",
    "else:\n",
    "    print(\"GROQ_API_KEY not set\")\n",
    "\n",
    "\n",
    "client = get_llm_client(LLM_CHOICE)\n",
    "if LLM_CHOICE == \"GROQ\":\n",
    "    MODEL = \"llama-3.3-70b-versatile\"\n",
    "else:\n",
    "    MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "print(f\"LLM_CHOICE: {LLM_CHOICE} - MODEL: {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfYrV_vOQIwY"
   },
   "source": [
    "## Main Code For LLM As A Judge For RAG (With Structured Outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rXc7OkcP47dF",
    "outputId": "4168cde7-bc7a-479e-edd2-13174281e538"
   },
   "outputs": [],
   "source": [
    "# Define Enum for scores\n",
    "class Score(str, Enum):\n",
    "    no_relevance = \"0\"\n",
    "    low_relevance = \"1\"\n",
    "    medium_relevance = \"2\"\n",
    "    high_relevance = \"3\"\n",
    "\n",
    "\n",
    "# Define a constant for the score description\n",
    "SCORE_DESCRIPTION = (\n",
    "    \"Score as a string between '0' and '3'. \"\n",
    "    \"0: No relevance/Not grounded/Irrelevant - The context/answer is completely unrelated or not based on the context. \"\n",
    "    \"1: Low relevance/Low groundedness/Somewhat relevant - The context/answer has minimal relevance or grounding. \"\n",
    "    \"2: Medium relevance/Medium groundedness/Mostly relevant - The context/answer is somewhat relevant or grounded. \"\n",
    "    \"3: High relevance/High groundedness/Fully relevant - The context/answer is highly relevant or grounded.\"\n",
    ")\n",
    "\n",
    "\n",
    "# Define separate classes for each criterion with detailed descriptions\n",
    "class ContextRelevance(BaseModel):\n",
    "    explanation: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Step-by-step reasoning explaining how the retrieved context aligns with the user's query. \"\n",
    "            \"Consider the relevance of the information to the query's intent and the appropriateness of the context \"\n",
    "            \"in providing a coherent and useful response.\"\n",
    "        ),\n",
    "    )\n",
    "    score: Score = Field(..., description=SCORE_DESCRIPTION)\n",
    "\n",
    "\n",
    "class AnswerRelevance(BaseModel):\n",
    "    explanation: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Step-by-step reasoning explaining how well the generated answer addresses the user's original query. \"\n",
    "            \"Consider the helpfulness and on-point nature of the answer, aligning with the user's intent and providing valuable insights.\"\n",
    "        ),\n",
    "    )\n",
    "    score: Score = Field(..., description=SCORE_DESCRIPTION)\n",
    "\n",
    "\n",
    "class Groundedness(BaseModel):\n",
    "    explanation: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Step-by-step reasoning explaining how faithful the generated answer is to the retrieved context. \"\n",
    "            \"Consider the factual accuracy and reliability of the answer, ensuring it is grounded in the retrieved information.\"\n",
    "        ),\n",
    "    )\n",
    "    score: Score = Field(..., description=SCORE_DESCRIPTION)\n",
    "\n",
    "\n",
    "class RAGEvaluation(BaseModel):\n",
    "    context_relevance: ContextRelevance = Field(\n",
    "        ...,\n",
    "        description=\"Evaluation of the context relevance to the query, considering how well the retrieved context aligns with the user's intent.\",\n",
    "    )\n",
    "    answer_relevance: AnswerRelevance = Field(\n",
    "        ...,\n",
    "        description=\"Evaluation of the answer relevance to the query, assessing how well the generated answer addresses the user's original query.\",\n",
    "    )\n",
    "    groundedness: Groundedness = Field(\n",
    "        ...,\n",
    "        description=\"Evaluation of the groundedness of the generated answer, ensuring it is faithful to the retrieved context.\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Function to evaluate RAG metrics\n",
    "def evaluate_rag(query: str, retrieved_context: str, generated_answer: str):\n",
    "    chat_response = client.chat.completions.parse(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a judge for evaluating a Retrieval-Augmented Generation (RAG) system. \"\n",
    "                    \"Evaluate the context relevance, answer relevance, and groundedness based on the following criteria: \"\n",
    "                    \"Provide a reasoning and a score as a string between '0' and '3' for each criterion. \"\n",
    "                    \"Context Relevance: How relevant is the retrieved context to the query? \"\n",
    "                    \"Answer Relevance: How relevant is the generated answer to the query? \"\n",
    "                    \"Groundedness: How faithful is the generated answer to the retrieved context?\"\n",
    "                ),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Query: {query}\\nRetrieved Context: {retrieved_context}\\nGenerated Answer: {generated_answer}\",\n",
    "            },\n",
    "        ],\n",
    "        response_format=RAGEvaluation,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return chat_response.choices[0].message.parsed\n",
    "\n",
    "\n",
    "# Example usage\n",
    "query = \"What are the benefits of renewable energy?\"\n",
    "retrieved_context = \"Renewable energy includes solar, wind, hydro, and geothermal energy, which are naturally replenished.\"\n",
    "generated_answer = \"Renewable energy sources like solar and wind are environmentally friendly and reduce carbon emissions.\"\n",
    "evaluation = evaluate_rag(query, retrieved_context, generated_answer)\n",
    "output = \"üèÜ RAG Evaluation:\"\n",
    "output += \"\\nCriteria: Context Relevance\\n\"\n",
    "output += f\"Reasoning: {evaluation.context_relevance.explanation}\\n\"\n",
    "output += f\"Score: {evaluation.context_relevance.score.value}/3\\n\"\n",
    "output += \"\\nCriteria: Answer Relevance\"\n",
    "output += f\"\\nReasoning: {evaluation.answer_relevance.explanation}\\n\"\n",
    "output += f\"\\nScore: {evaluation.context_relevance.score.value}/3\\n\"\n",
    "output += \"\\nCriteria: Groundedness\"\n",
    "output += f\"\\nReasoning: {evaluation.groundedness.explanation}\\n\"\n",
    "output += f\"\\nScore: {evaluation.groundedness.score.value}/3\\n\"\n",
    "# Print the evaluation\n",
    "# print(\"üèÜ RAG Evaluation:\")\n",
    "# print(\"\\nCriteria: Context Relevance\")\n",
    "# print(f\"Reasoning: {evaluation.context_relevance.explanation}\")\n",
    "# print(f\"Score: {evaluation.context_relevance.score.value}/3\")\n",
    "\n",
    "# print(\"\\nCriteria: Answer Relevance\")\n",
    "# print(f\"Reasoning: {evaluation.answer_relevance.explanation}\")\n",
    "# print(f\"Score: {evaluation.answer_relevance.score.value}/3\")\n",
    "\n",
    "# print(\"\\nCriteria: Groundedness\")\n",
    "# print(f\"Reasoning: {evaluation.groundedness.explanation}\")\n",
    "# print(f\"Score: {evaluation.groundedness.score.value}/3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">üèÜ RAG Evaluation:\n",
       "Criteria: Context Relevance\n",
       "Reasoning: The retrieved context provides a definition of renewable energy and lists its types, which is relevant \n",
       "to the query about the benefits of renewable energy. However, it does not directly address the benefits themselves,\n",
       "which is the main focus of the query. Therefore, while it is somewhat relevant, it does not fully meet the user's \n",
       "intent.\n",
       "Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "\n",
       "Criteria: Answer Relevance\n",
       "Reasoning: The generated answer directly addresses the query by stating that renewable energy sources are \n",
       "environmentally friendly and reduce carbon emissions, which are indeed benefits of renewable energy. This makes the\n",
       "answer relevant to the user's question about the benefits.\n",
       "\n",
       "Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "\n",
       "Criteria: Groundedness\n",
       "Reasoning: The generated answer is grounded in the context provided, as it mentions solar and wind energy, which \n",
       "are included in the retrieved context. The benefits mentioned in the answer <span style=\"font-weight: bold\">(</span>environmental friendliness and \n",
       "reduction of carbon emissions<span style=\"font-weight: bold\">)</span> are consistent with the characteristics of renewable energy. Thus, the answer is \n",
       "faithful to the retrieved context.\n",
       "\n",
       "Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "üèÜ RAG Evaluation:\n",
       "Criteria: Context Relevance\n",
       "Reasoning: The retrieved context provides a definition of renewable energy and lists its types, which is relevant \n",
       "to the query about the benefits of renewable energy. However, it does not directly address the benefits themselves,\n",
       "which is the main focus of the query. Therefore, while it is somewhat relevant, it does not fully meet the user's \n",
       "intent.\n",
       "Score: \u001b[1;36m2\u001b[0m/\u001b[1;36m3\u001b[0m\n",
       "\n",
       "Criteria: Answer Relevance\n",
       "Reasoning: The generated answer directly addresses the query by stating that renewable energy sources are \n",
       "environmentally friendly and reduce carbon emissions, which are indeed benefits of renewable energy. This makes the\n",
       "answer relevant to the user's question about the benefits.\n",
       "\n",
       "Score: \u001b[1;36m2\u001b[0m/\u001b[1;36m3\u001b[0m\n",
       "\n",
       "Criteria: Groundedness\n",
       "Reasoning: The generated answer is grounded in the context provided, as it mentions solar and wind energy, which \n",
       "are included in the retrieved context. The benefits mentioned in the answer \u001b[1m(\u001b[0menvironmental friendliness and \n",
       "reduction of carbon emissions\u001b[1m)\u001b[0m are consistent with the characteristics of renewable energy. Thus, the answer is \n",
       "faithful to the retrieved context.\n",
       "\n",
       "Score: \u001b[1;36m3\u001b[0m/\u001b[1;36m3\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console.print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Now write the file\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(output)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "eval-framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
