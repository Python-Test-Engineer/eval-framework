{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHmYsdct7NoD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from evidently import Dataset\n",
        "from evidently import DataDefinition\n",
        "from evidently.descriptors import *\n",
        "\n",
        "from evidently import Report\n",
        "from evidently.presets import TextEvals\n",
        "from evidently.metrics import *\n",
        "from evidently.tests import *\n",
        "from rich.console import Console\n",
        "from rich.console import Console\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv, find_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "console = Console()\n",
        "\n",
        "load_dotenv(find_dotenv(), override=True)\n",
        "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
        "GROQ_API_KEY = os.environ[\"GROQ_API_KEY\"]\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "console.print(\"[cyan]Starting...[/]\")\n",
        "\n",
        "\n",
        "def get_llm_client(llm_choice):\n",
        "    if llm_choice == \"GROQ\":\n",
        "        client = OpenAI(\n",
        "            base_url=\"https://api.groq.com/openai/v1\",\n",
        "            api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
        "        )\n",
        "        return client\n",
        "    elif llm_choice == \"OPENAI\":\n",
        "        load_dotenv()  # load environment variables from .env fil\n",
        "        client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "        return client\n",
        "    else:\n",
        "        raise ValueError(\"Invalid LLM choice. Please choose 'GROQ' or 'OPENAI'.\")\n",
        "\n",
        "\n",
        "LLM_CHOICE = \"OPENAI\"\n",
        "# LLM_CHOICE = \"GROQ\"\n",
        "\n",
        "if OPENAI_API_KEY:\n",
        "    console.print(\n",
        "        f\"[green]✅ OPENAI_API_KEY exists and begins {OPENAI_API_KEY[:14]}...[/]\"\n",
        "    )\n",
        "else:\n",
        "    console.print(\"[red bold]❌ OPENAI_API_KEY not set[/]\")\n",
        "\n",
        "if GROQ_API_KEY:\n",
        "    console.print(f\"[green]✅ GROQ_API_KEY exists and begins {GROQ_API_KEY[:14]}...[/]\")\n",
        "\n",
        "else:\n",
        "    console.print(\"[red bold]❌ GROQ_API_KEY not set[/]\")\n",
        "\n",
        "\n",
        "client = get_llm_client(LLM_CHOICE)\n",
        "if LLM_CHOICE == \"GROQ\":\n",
        "    MODEL = \"llama-3.3-70b-versatile\"\n",
        "else:\n",
        "    MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "console.print(f\"[green]✅ LLM_CHOICE: {LLM_CHOICE} - MODEL: {MODEL}[/]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HVOY2soztLN"
      },
      "source": [
        "# Retrieval - Single context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWK1DBORIL_a"
      },
      "outputs": [],
      "source": [
        "# [\"Question\", \"Ground_Truth\", \"Answer\"]\n",
        "\n",
        "synthetic_data = [\n",
        "    [\n",
        "        \"What is Langgraph?\",\n",
        "        \"LangGraph is a library for building stateful, multi-actor applications with Large Language Models (LLMs), built on top of LangChain. It's designed to create complex, graph-based workflows where different components can interact and maintain state across multiple steps.\",\n",
        "        \"LangGraph is a framework for building LLM apps. It has a foundation of Langchain\",\n",
        "    ],\n",
        "    [\n",
        "        \"What is MCP?\",\n",
        "        \"MCP stands for Model Context Protocol - it's an open standard developed by Anthropic for connecting AI assistants to data sources and tools in a secure, standardized way.\",\n",
        "        \"MCP acts as a bridge between AI models (like Claude) and external systems, allowing the AI to access and use data from various sources\",\n",
        "    ],\n",
        "]\n",
        "\n",
        "\n",
        "columns = [\"Question\", \"Ground_Truth\", \"Answer\"]\n",
        "\n",
        "\n",
        "synthetic_df = pd.DataFrame(synthetic_data, columns=columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hysc1JRsz285"
      },
      "outputs": [],
      "source": [
        "pd.set_option(\"display.max_colwidth\", None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "synthetic_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv4ANZCsLz3C"
      },
      "source": [
        "## ContextQuality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5ccKrpkJs8U"
      },
      "outputs": [],
      "source": [
        "context_based_evals = Dataset.from_pandas(\n",
        "    pd.DataFrame(synthetic_df),\n",
        "    data_definition=DataDefinition(\n",
        "        text_columns=[\"Question\", \"Ground_Truth\", \"Answer\"],\n",
        "    ),\n",
        "    descriptors=[\n",
        "        ContextQualityLLMEval(\"Ground_Truth\", question=\"Question\"),\n",
        "    ],\n",
        ")\n",
        "context_based_evals.as_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrZ5ftRuqof_"
      },
      "source": [
        "## ContextRelevance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkRKfjssqn-E"
      },
      "outputs": [],
      "source": [
        "context_based_evals = Dataset.from_pandas(\n",
        "    pd.DataFrame(synthetic_df),\n",
        "    data_definition=DataDefinition(\n",
        "        text_columns=[\"Question\", \"Ground_Truth\", \"Answer\"],\n",
        "    ),\n",
        "    descriptors=[\n",
        "        ContextRelevance(\n",
        "            \"Question\",\n",
        "            \"Ground_Truth\",\n",
        "            output_scores=True,\n",
        "            aggregation_method=\"hit\",\n",
        "            method=\"llm\",\n",
        "            alias=\"Hit\",\n",
        "        )\n",
        "    ],\n",
        ")\n",
        "context_based_evals.as_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSWUKa0TqXA9"
      },
      "source": [
        "## ContextRelevance, Hit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Toga39jdRg2d"
      },
      "outputs": [],
      "source": [
        "context_based_evals = Dataset.from_pandas(\n",
        "    pd.DataFrame(synthetic_df),\n",
        "    data_definition=DataDefinition(\n",
        "        text_columns=[\"Question\", \"Ground_Truth\", \"Answer\"],\n",
        "    ),\n",
        "    descriptors=[\n",
        "        ContextRelevance(\n",
        "            \"Question\",\n",
        "            \"Ground_Truth\",\n",
        "            output_scores=True,\n",
        "            aggregation_method=\"hit\",\n",
        "            method=\"llm\",\n",
        "            alias=\"Hit\",\n",
        "        )\n",
        "    ],\n",
        ")\n",
        "context_based_evals.as_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpFuF85UqYp4"
      },
      "source": [
        "## ContextRelevance, Mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGyJuHNomT0u"
      },
      "outputs": [],
      "source": [
        "context_based_evals = Dataset.from_pandas(\n",
        "    pd.DataFrame(synthetic_df),\n",
        "    data_definition=DataDefinition(\n",
        "        text_columns=[\"Question\", \"Ground_Truth\", \"Answer\"],\n",
        "    ),\n",
        "    descriptors=[\n",
        "        ContextRelevance(\n",
        "            \"Question\",\n",
        "            \"Ground_Truth\",\n",
        "            output_scores=True,\n",
        "            aggregation_method=\"mean\",\n",
        "            method=\"llm\",\n",
        "            alias=\"Relevance\",\n",
        "        )\n",
        "    ],\n",
        ")\n",
        "context_based_evals.as_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0I2wIVKJgeL"
      },
      "source": [
        "# Generation - ground truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpKqYYfhxqoF"
      },
      "outputs": [],
      "source": [
        "context_based_evals = Dataset.from_pandas(\n",
        "    pd.DataFrame(synthetic_df),\n",
        "    data_definition=DataDefinition(\n",
        "        text_columns=[\"Question\", \"Answer\", \"Ground_Truth\"],\n",
        "    ),\n",
        "    descriptors=[\n",
        "        CorrectnessLLMEval(\"Answer\", target_output=\"Ground_Truth\"),\n",
        "        BERTScore(columns=[\"Answer\", \"Ground_Truth\"], alias=\"BERTScore\"),\n",
        "        SemanticSimilarity(\n",
        "            columns=[\"Answer\", \"Ground_Truth\"], alias=\"Semantic Similarity\"\n",
        "        ),\n",
        "    ],\n",
        ")\n",
        "context_based_evals.as_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMDFkWRkJZSS"
      },
      "source": [
        "# Generation - open-ended"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LhQR1Rkzsxx"
      },
      "outputs": [],
      "source": [
        "context_based_evals = Dataset.from_pandas(\n",
        "    pd.DataFrame(synthetic_df),\n",
        "    data_definition=DataDefinition(\n",
        "        text_columns=[\"Question\", \"Ground_Truth\", \"Answer\"],\n",
        "    ),\n",
        "    descriptors=[FaithfulnessLLMEval(\"Answer\", context=\"Ground_Truth\")],\n",
        ")\n",
        "context_based_evals.as_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU5acjQM3yRJ"
      },
      "source": [
        "# Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqqpwG6hPZaA"
      },
      "source": [
        "Combine ContextQuality and faithfulness:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-wLZxWH5Uw2"
      },
      "outputs": [],
      "source": [
        "context_based_evals = Dataset.from_pandas(\n",
        "    pd.DataFrame(synthetic_df),\n",
        "    data_definition=DataDefinition(\n",
        "        text_columns=[\"Question\", \"Ground_Truth\", \"Answer\"],\n",
        "    ),\n",
        "    descriptors=[\n",
        "        FaithfulnessLLMEval(\"Answer\", context=\"Ground_Truth\"),\n",
        "        ContextQualityLLMEval(\"Ground_Truth\", question=\"Question\"),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# context_based_evals.as_dataframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueQRqHC15oyl"
      },
      "outputs": [],
      "source": [
        "report = Report([TextEvals()])\n",
        "\n",
        "my_eval = report.run(context_based_evals, None)\n",
        "my_eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku23maBfEnUd"
      },
      "source": [
        "# Add Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA1gWZjkj0nC"
      },
      "outputs": [],
      "source": [
        "report = Report(\n",
        "    [\n",
        "        TextEvals(),\n",
        "        CategoryCount(column=\"Faithfulness\", category=\"UNFAITHFUL\", tests=[eq(0)]),\n",
        "        CategoryCount(column=\"ContextQuality\", category=\"INVALID\", tests=[eq(0)]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "my_eval = report.run(context_based_evals, None)\n",
        "my_eval"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "eval-framework",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
