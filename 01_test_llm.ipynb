{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "466c4606",
   "metadata": {},
   "source": [
    "<img src=\"./images/01-takeaways.png\" width=700px>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "from rich.console import Console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbd2af16",
   "metadata": {},
   "outputs": [],
   "source": [
    "console = Console()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-it37ct...\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from a file called .env\n",
    "# There is a sample file .env.sample - rename this to .env and put your API keys there.\n",
    "# https://console.groq.com/login has a free tier that uses the same interface as OpenAI\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:14]}...\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a90e8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model selected: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"gpt-4o-mini\"\n",
    "print(f\"Model selected: {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0949f018",
   "metadata": {},
   "source": [
    "For demo, we will create our own OpenAI request class but in future we will use the OpenAI library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f6ba59",
   "metadata": {},
   "source": [
    "\n",
    "Temperature - The OpenAI API incorporates a hyperparameter known as temperature that affects the computation of token probabilities when generating output through the large language model. The temperature value ranges from 0 to 2, with lower values indicating greater determinism and higher values indicating more randomness. Default is 1.\n",
    "\n",
    "Low Temperature:\n",
    "\n",
    "The bag is full of mostly blue marbles, with a few red and green. Low temperature means you're very likely to pull a blue marble, but you might occasionally get a red or green.\n",
    "\n",
    "High Temperature:\n",
    "\n",
    "The bag is filled with a mix of colors, and all colors are equally likely. High temperature means you're equally likely to pull any color, including the less common ones.\n",
    "\n",
    "<img src=\"./images/temperature.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1226148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are essentially making a POST request to one endpoint and we create a convenience Class to pass in prompts, temperature and model.\n",
    "\n",
    "\n",
    "class MyCustomOpenAI:\n",
    "\n",
    "    def __init__(self, system_prompt=None, temperature=1.0, model=MODEL):\n",
    "        self.model_endpoint = \"https://api.openai.com/v1/chat/completions\"\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.system_prompt = system_prompt\n",
    "        self.api_key = openai_api_key\n",
    "        self.headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "        }\n",
    "\n",
    "    # Note that other than the API key to be authenticated, we pass nor reference to previous queries. Each request is STATELESS and is independent of any other request.\n",
    "\n",
    "    # That is why we append previous output to new queries so that the LLM has the full context or 'picture'.\n",
    "\n",
    "    # OpenAI was trained with 'messages' in the form of a list of messages - SYSTEM, USER, ASSISTANT - so using this is most effective.\n",
    "\n",
    "    # SYSTEM will contain the 'character' and instruction set of the Agent, USER will be the input to the Agent and ASSISTANT will be the output.\n",
    "\n",
    "    # TAKEAWAY: WWe are appending the last output to the list of 'messages' so that the Agent has knowledge of the context - the history of the conversation.\n",
    "\n",
    "    def generate_text(self, prompt):\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            \"stream\": False,  # no streaming or output\n",
    "            \"temperature\": self.temperature,\n",
    "        }\n",
    "        # Use HTTP POST method from Requests library\n",
    "\n",
    "        # In future examples we will use OpenAI library instead of requests directly but this is for demo purposes.\n",
    "        response = requests.post(\n",
    "            url=self.model_endpoint, headers=self.headers, data=json.dumps(payload)\n",
    "        ).json()\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "548bb3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an instance of MyCustomOpenAI and pass in a request - just a convenience wrapper of making a REST API call.\n",
    "\n",
    "# System Prompt sets up the character of the AI Agent and can be considered to be the API route we are creating and it it will be in Natural Language - explained later...\n",
    "\n",
    "client = MyCustomOpenAI(\n",
    "    model=MODEL,\n",
    "    system_prompt=\"You give concise answers to questions with no more than 200 characters\",\n",
    "    temperature=0.0,  # as deterministic as possible\n",
    ")\n",
    "\n",
    "response = client.generate_text(\n",
    "    \"What is an the difference between Pydantic.ai and Lnagchain?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b39feb11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'chatcmpl-BvQvubGnEVk42VJRSr0ifvWcBgAI9'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'object'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'chat.completion'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'created'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1753027926</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-mini-2024-07-18'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'choices'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'index'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Pydantic is a data validation and settings management library for Python, while </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Langchain is a framework for building applications with language models.'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'refusal'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'annotations'</span>: <span style=\"font-weight: bold\">[]</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'usage'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">39</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">66</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens_details'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'cached_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'audio_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens_details'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'reasoning_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'audio_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'accepted_prediction_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'rejected_prediction_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'service_tier'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'system_fingerprint'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'id'\u001b[0m: \u001b[32m'chatcmpl-BvQvubGnEVk42VJRSr0ifvWcBgAI9'\u001b[0m,\n",
       "    \u001b[32m'object'\u001b[0m: \u001b[32m'chat.completion'\u001b[0m,\n",
       "    \u001b[32m'created'\u001b[0m: \u001b[1;36m1753027926\u001b[0m,\n",
       "    \u001b[32m'model'\u001b[0m: \u001b[32m'gpt-4o-mini-2024-07-18'\u001b[0m,\n",
       "    \u001b[32m'choices'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'index'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "            \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m,\n",
       "                \u001b[32m'content'\u001b[0m: \u001b[32m'Pydantic is a data validation and settings management library for Python, while \u001b[0m\n",
       "\u001b[32mLangchain is a framework for building applications with language models.'\u001b[0m,\n",
       "                \u001b[32m'refusal'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                \u001b[32m'annotations'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'usage'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m39\u001b[0m,\n",
       "        \u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m27\u001b[0m,\n",
       "        \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m66\u001b[0m,\n",
       "        \u001b[32m'prompt_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'cached_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m'audio_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'completion_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'reasoning_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "            \u001b[32m'audio_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "            \u001b[32m'accepted_prediction_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "            \u001b[32m'rejected_prediction_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'service_tier'\u001b[0m: \u001b[32m'default'\u001b[0m,\n",
       "    \u001b[32m'system_fingerprint'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can drill down into 'response' to get our choice of data\n",
    "# The 'answer' is in the message with 'role': 'assistant'\n",
    "console.print(response)\n",
    "# print(response[\"choices\"][0])\n",
    "# print(response[\"choices\"][0][\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "905b1fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Pydantic is a data validation and settings management library for Python, '\n",
      " 'while Langchain is a framework for building applications with language '\n",
      " 'models.')\n"
     ]
    }
   ],
   "source": [
    "pprint(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c8ee55",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-evaluation-framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
